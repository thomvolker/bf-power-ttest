---
title: "Power Bayesian one-sample t-test"
format: html
---

# Bayesian null hypothesis evaluation

We consider the case of a one-sample $t$-test. To fix notation, let $X_1, \ldots, X_n$ be a sample of size $n$ from a normal distribution with unknown mean $\mu$ and unknown variance $\sigma^2$. We are interested in testing the null hypothesis $H_0: \mu = \mu_0$ against the alternative hypothesis $H_1: \mu \neq \mu_0$.
Moreover, we use the approximate adjusted fractional Bayes factor (AAFBF; [Gu, Mulder & Hoijtink, 2017](https://bpspsychub.onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12110)), which implies that we approximate the posterior distribution of the parameters by a normal distribution with mean and variance equal to the (unbiased) maximum likelihood estimates. 
Using this method, the Bayes factor is given by
$$
\begin{aligned}
BF_{0,u} &= \frac{f_0}{c_0} \\
&= \frac{P(\mu = \mu_0 | X, \pi_0)}{P(\mu = \mu_0 | \pi_0)}, 
\end{aligned}
$$
where $f_0$ denotes the fit of the posterior to the hypothesis of interest, $c_0$ denotes the complexity of the hypothesis of interest, $X$ denotes the data, and $\pi_0$ denotes the prior distribution of the parameters under the null hypothesis.
Given our normal approximation and the definition of the fractional adjusted Bayes factor, the prior is given by
$$
\pi_0 = \mathcal{N}
\Big(\mu_0, \frac{1}{b} \frac{\hat{\sigma}^2}{n}\Big),
$$
where $b$ is the fraction of information in the data used to specify the prior distribution, typically specified as $J/n$, and $\hat{\sigma}^2$ is the unbiased estimate of the variance of the data.
For the one-sample $t$-test, we set $J=1$ and thus $b = \frac{1}{n}$, such that we have
$$
\pi_0 = \mathcal{N}(\mu_0, \hat{\sigma}^2).
$$
Additionally, the posterior distribution is given by
$$
\pi = \mathcal{N}\Big(\hat{\mu},  \frac{\hat{\sigma}^2}{n}\Big).
$$
Without loss of generality, we can rescale the data by subtracting the hypothesized mean and dividing by the standard error $\frac{\hat{\sigma}}{\sqrt{n}}$, such that we have
$$
\begin{aligned}
\pi_0 &= \mathcal{N}(0, n), \\
\pi &= \mathcal{N}(\tilde{\mu}, 1),
\end{aligned}
$$
where $\tilde{\mu} = \frac{\hat{\mu} - \mu_0}{\hat{\sigma}/\sqrt{n}}$.

Accordingly, the approximate adjusted fractional Bayes factor is given by
$$
\begin{aligned}
BF_{0,u} &= \frac{
\frac{1}{\sqrt{2\pi\frac{\hat{\sigma}^2}{n}}}
\exp \Big\{-\frac{(\hat{\mu} - \mu_0)^2}{2\frac{\hat{\sigma}^2}{n}} \Big\}
}{
\frac{1}{\sqrt{2\pi\hat{\sigma}^2}}
\exp \Big\{-\frac{(\mu_0-\mu_0)^2}{2\hat{\sigma}^2} \Big\}
} \\
&= \frac{
\frac{1}{\sqrt{2\pi}} \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\}
}{
\frac{1}{\sqrt{2 \pi n}}
} \\
&= \sqrt{n} \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\}.
\end{aligned}
$$ {#eq-bf-simple}

To show this, consider the following example, where we have a sample of size $n = 20$ drawn from a normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. We are interested in testing the null hypothesis $H_0: \mu = 0$ against the alternative hypothesis $H_1: \mu \neq 0$. 

```{r}
set.seed(123)
n <- 20
mu <- 0
sd <- 1
x <- rnorm(n, mu, sd)
(mux <- mean(x))
(sdx <- sd(x))

J <- 1

fit <- bain::t_test(x)
bain::bain(fit, "x=0", fraction = J)

f0 <- 1 / sqrt(2 * pi * sdx^2 / n) * exp(-mux^2 / (2 * sdx^2 / n))
c0 <- 1 / sqrt(2 * pi * sdx^2/J) * exp(-0^2 / (2 * sdx^2/J))

f0 / c0

# of course, we can also make use of the in-build functionality
dnorm(mux, 0, sdx / sqrt(n)) / dnorm(0, 0, sdx/J)
```

Alternatively, we can use the simplified expression like so.


```{r}
mutilde <- mux / (sdx/sqrt(n))
f0 <- 1 / sqrt(2 * pi) * exp(-mutilde^2 / 2)
c0 <- 1 / sqrt(2 * pi * n)
f0/c0

sqrt(n) * exp(-mutilde^2/2)
```

It is easy see that the Bayes factor is only a function of the scaled sample mean. 

## Power of a Bayesian one-sided t-test by simulation


To obtain the power of a test with sample size $n$, we need to calculate the 
probability that the Bayes factor is larger than a certain value $Q$. To do this
we generate `nsim = 100000000` samples of size $n$ from a normal distribution,
calculate the Bayes factor for the null hypothesis $\mu = 0$ against the
unconstrained alternative hypothesis, and calculate how often the Bayes factor
exceeds the critical value $T$. For now, suppose $Q = 3$. 


```{r}
library(pbapply)

nsim <- 10000000
Q <- 3

n   <- 20
mu  <- 0
sd  <- 1
mu0 <- 0

simfunc <- function(n = 20, mu = 0, sd = 1, J = 1, mu0 = 0) {
  x <- rnorm(n, mu, sd)
  mu <- mean(x)
  se <- sqrt(var(x)/n)
  mutilde <- (mu - mu0) / se
  BF <- sqrt(n/J) * exp(-mutilde^2/2)
  BF
}

pboptions(type = "timer")
cl <- parallel::makeCluster(16)
parallel::clusterExport(cl, c("simfunc", "n", "mu", "sd", "J", "mu0"))
bfs <- pbreplicate(nsim, simfunc(n = n, mu = mu, sd = sd, mu0 = mu0), cl = cl)
parallel::stopCluster(cl)

mean(bfs > Q)
```

Hence, the power of the test is approximately $`r round(mean(bfs > Q), 3)`$. 


## Power of a Bayesian one-sided t-test by analytical calculation

Now, noting that the Bayes factor only depends on the scaled sample mean, we can calculate the power analytically. Importantly, note that the distribution of the scaled sample mean with unknown variance follows a non-central $t$-distribution with $n-1$ degrees of freedom, and non-centrality parameter $\delta = (\mu - \mu_0) / (\sigma/\sqrt{n})$. This scaling of the non-centrality parameter arises because we center the data to the hypothesized value and scale by it's standard error. Therefore, we have to apply the same scaling to the population mean $\mu$.
If the variance was known, the distribution would be a normal distribution with mean $\mu / (\sigma/\sqrt{n})$ and standard deviation $1$.

The next step is to calculate for which values of $\tilde{\mu}$ the Bayes factor exceeds the threshold $Q$.
Per @eq-bf-simple, we have that
$$
\begin{aligned}
Q &\leq \sqrt{n} \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\} \\
\frac{Q}{\sqrt{n}} &\leq \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\} \\
\log \Big(\frac{Q}{\sqrt{n}} \Big) &\leq -\frac{\tilde{\mu}^2}{2} \\
2 \log \Big(\frac{\sqrt{n}}{Q} \Big) &\leq \tilde{\mu}^2 \\
\tilde\mu &\in \Bigg[-\sqrt{2 \log \Big(\frac{\sqrt{n}}{Q} \Big)}, \sqrt{2 \log \Big(\frac{\sqrt{n}}{Q} \Big)}\Bigg].
\end{aligned}
$$
Hence, as long as $\tilde\mu$ falls within the interval $\Big[-\sqrt{2 \log \Big(\frac{\sqrt{n}}{Q} \Big)}, \sqrt{2 \log \Big(\frac{\sqrt{n}}{Q} \Big)}\Big]$, the Bayes factor will exceed the threshold $Q$.

The final step is to calculate the probability with which the scaled sample mean falls within this interval. This can be done by calculating the cumulative distribution function of the non-central $t$-distribution with $n-1$ degrees of freedom and non-centrality parameter $\delta = \mu / (\sigma/\sqrt{n})$ at the upper and lower bounds of the interval defined by $\mu_\text{critical}$.

```{r}
mu_critical <- sqrt(2 * log(sqrt(n) / Q))
delta <- (0 - 0) / (1/sqrt(n))

pt(mu_critical, n-1, delta) - pt(-mu_critical, n-1, delta)
```

## A normal approximation

In case the distribution of the statistic of interest is not known exactly, but can be approximated by a normal distribution, we can also use the cumulative normal distribution function. Note that this would not alter the critical value of the sample mean $\mu_\text{critical}$, but solely alters the sampling distribution of the mean. In this case, we would have that the scaled sample mean follows a normal distribution with mean $\mu / (\sigma/\sqrt{n})$ and standard deviation $1$. 

```{r}
pnorm(mu_critical, delta, sd) - pnorm(-mu_critical, delta, sd)
```

## The power for any $J$

So far, we assumed that $J = 1$, and thus the fraction of information used in the training data equals $b = \frac{1}{n}$. However, using a different value for $J$ would yield a different Bayes factor value. Hence, this value alters the calculation of the critical value $Q$, but not the sampling distribution of the mean. Moreover, the value of $J$ only alters the prior distribution, which then yields
$$
\pi_0 = \mathcal{N}
\Big(\mu_0, \frac{\hat{\sigma}^2}{J}\Big).
$$
Accordingly, the Bayes factor equals
$$
\begin{aligned}
BF_{0,u} &= \frac{
\frac{1}{\sqrt{2\pi\frac{\hat{\sigma}^2}{n}}}
\exp \Big\{-\frac{(\hat{\mu} - \mu_0)^2}{2\frac{\hat{\sigma}^2}{n}} \Big\}
}{
\frac{1}{\sqrt{2\pi\hat{\sigma}^2/J}}
\exp \Big\{-\frac{(\mu_0-\mu_0)^2}{2\frac{\hat{\sigma}^2}{J}} \Big\}
} \\
&= \frac{
\frac{1}{\sqrt{2\pi}} \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\}
}{
\frac{1}{\sqrt{2\pi \frac{n}{J}}}
} \\
&= \sqrt{\frac{n}{J}} \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\},
\end{aligned}
$$ {#eq-bf-simple-J}
and we can define the critical value of the scaled sample mean as $\mu_\text{critical}$ for which the Bayes factor exceeds $Q$ as
$$
\begin{aligned}
Q &\leq \sqrt{\frac{n}{J}} \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\} \\
\frac{Q}{\sqrt{n/J}} &\leq \exp \Big\{-\frac{\tilde{\mu}^2}{2} \Big\} \\
\log \Big(\frac{Q}{\sqrt{n/J}} \Big) &\leq -\frac{\tilde{\mu}^2}{2} \\
2 \log \Big(\frac{\sqrt{n/J}}{Q} \Big) &\leq \tilde{\mu}^2 \\
\tilde\mu &\in \Bigg[-\sqrt{2 \log \Big(\frac{\sqrt{n/J}}{Q} \Big)}, \sqrt{2 \log \Big(\frac{\sqrt{n/J}}{Q} \Big)}\Bigg].
\end{aligned}
$$


## Examples

### The BFpower function

Compiling the above equations in a single `R` function yields the following code.

```{r}
BFpower <- function(n, mu, sd, J, mu0, Q) {
  mu_critical <- sqrt(2 * log(sqrt(n/J) / Q))
  delta <- (mu - mu0) / (sd/sqrt(n))
  p <- pt(mu_critical, n-1, delta) - pt(-mu_critical, n-1, delta)
  return(p)
}
```

### Changing $J$

Increasing $J$, we put a more informative prior on the parameter of interest. Accordingly, the Bayes factor in favor of the null hypothesis decreases. This is because the prior distribution is more concentrated around the null hypothesis, and thus the data are more likely to provide evidence against the null hypothesis compared to our a priori beliefs.


```{r}
Q <- 3
n   <- 20
J   <- 2
mu  <- 0
sd  <- 1
mu0 <- 0

cl <- parallel::makeCluster(16)
parallel::clusterExport(cl, c("simfunc", "n", "mu", "sd", "J", "mu0"))
bfs <- pbreplicate(nsim, simfunc(n = n, mu = mu, sd = sd, J = J, mu0 = mu0), cl = cl)
parallel::stopCluster(cl)

mean(bfs > Q)
BFpower(n, mu, sd, J, mu0, Q)
```

We indeed observe that the power for finding a Bayes factor that exceeds the threshold $Q \geq 3$ is smaller when $J = 2$ compared to $J = 1$.

### Changing $Q$, $n$, $\mu$ and $\sigma$

```{r}
Q   <- 2
n   <- 30
mu  <- 1
sd  <- 2
mu0 <- 0

cl <- parallel::makeCluster(16)
parallel::clusterExport(cl, c("simfunc", "n", "mu", "sd", "J", "mu0"))
bfs <- pbreplicate(nsim, simfunc(n = n, mu = mu, sd = sd, J = J, mu0 = mu0), cl = cl)
parallel::stopCluster(cl)

mean(bfs > Q)
BFpower(n, mu, sd, J, mu0, Q)
```


### Changing the hypothesized value $\mu_0$

```{r}
Q   <- 2
n   <- 30
mu  <- 1
sd  <- 2
mu0 <- 1

cl <- parallel::makeCluster(16)
parallel::clusterExport(cl, c("simfunc", "n", "mu", "sd", "J", "mu0"))
bfs <- pbreplicate(nsim, simfunc(n = n, mu = mu, sd = sd, J = J, mu0 = mu0), cl = cl)
parallel::stopCluster(cl)

mean(bfs > Q)
BFpower(n, mu, sd, J, mu0, Q)
```



# Bayesian informative hypothesis evaluation

To evaluate the power of an informative hypothesis against the unconstrained hypothesis, we can use a similar approach. However, if the informative hypothesis of interest is an inequality-constrained hypothesis, the Bayes factor is defined in a slightly different way. In this case, we can make use of the encompassing prior approach put forward by [Klugkist et al. (2005)](https://psycnet.apa.org/record/2005-16136-009). Under this set-up, the Bayes factor for the inequality-constrained hypothesis evaluated against the unconstrained hypothesis is given by
$$
\begin{aligned}
BF_{i,u} &= \frac{f_i}{c_i} \\
&= 
\frac{
\int_{\mu \in M_i} \pi(\mu|X)
}{
\int_{\mu \in M_i} \pi_0(\mu)
},
\end{aligned}
$$
where $M_i$ denotes the parameter space under the inequality-constrained hypothesis, $\pi(\mu|X)$ denotes the posterior distribution of the mean given the data (marginalized over the variance), and $\pi_0(\mu)$ denotes the prior distribution of the mean (also marginalized over the variance). 

## Bayesian one-sided hypothesis evaluation

First, assume that the hypothesis of interest is a one-sided hypothesis, in the sense that we consider $H_i: \mu > \mu_0$ or $H_{i'}: \mu < \mu_0$. In this setting, centering the prior distribution at the hypothesized value yields a complexity of $c_i = 0.5$, and thus we have that $BF_{i,u} = 2 \cdot f_i$. From this observation, it follows that the maximum Bayes factor equals $BF_{i,u}^\max=2$, whereas the minimum Bayes factor equals $BF_{i,u}^\min=0$. Moreover, since a normal approximation is used to the posterior distribution of the parameters, we have that
$$
\begin{aligned}
BF_{i,u} &= 2 \int_{\mu \in M_i} 
\frac{1}{\sqrt{2\pi \frac{\hat\sigma^2}{n}}} 
\exp \Big\{ -\frac{(\hat\mu - \mu_0)^2}{2 \frac{\hat\sigma^2}{n}} \Big\} d\mu,\\
&= 2 \int_{\mu \in M_i} \frac{1}{\sqrt{2 \pi}} \exp \Big\{ -\frac{\tilde\mu^2}{2} \Big\} d\mu,\\
&= 2 \cdot (\Phi(\sup(M_i)) - \Phi(\inf(M_i))),
\end{aligned}
$$
where $\Phi$ denotes the cumulative distribution function of the normal distribution with mean $\tilde\mu$ and variance $1$, and $\sup(M_i)$ and $\inf(M_i)$ denote the upper and lower bounds of the parameter space under the inequality-constrained hypothesis, respectively. Assuming that the hypothesis of interest is $H_i: \mu > x$ or $H_{i'}: \mu < x$, with the prior distribution centered at $x$, we either have that $\Phi(\sup(M_i)) = 1$ or $\Phi(\inf(M_i)) = 0$, respectively. Hence, the critical value for $\tilde\mu$ that corresponds to a Bayes factor of $Q$ is given by
$$
\begin{aligned}
\mu_\text{critical} = \pm \Phi^{-1}(Q/2),
\end{aligned}
$$
where the $\pm$ sign depends on whether the hypothesis of interest is $H_i$ or $H_{i'}$ and $\Phi^{-1}$ denotes the inverse of the cumulative normal distribution.
The next step is to obtain the power by quantifying the probability that 
Accordingly, the power of the test is given by the probability that $\tilde\mu > \mu_\text{critical}$. Again, this probability is given by the cumulative distribution function of the non-central $t$-distribution with $n-1$ degrees of freedom and non-centrality parameter $\delta = (\mu-\mu_0)/(\sigma^2/n)$.


```{r}
Q <- 1.5

simfunc <- function(n, mu = 0, sd = 1, mu0 = 0) {
  x <- rnorm(n, mu, sd)
  mux <- mean(x)
  varx <- var(x)
  mutilde <- (mux-mu0) / sqrt(varx/n)
  pnorm(0, mutilde, 1, lower.tail = FALSE) / 
    pnorm(0, 0, sqrt(n), lower.tail = FALSE)
}

bain::bain(fit, "x > 0", fraction = J)

mutilde <- (mean(x) - 0) / sqrt(var(x)/length(x))
pnorm(0, mutilde, 1, lower.tail = FALSE) / 
  pnorm(0, 0, n, lower.tail = FALSE)

cl <- parallel::makeCluster(16)
parallel::clusterExport(cl, "simfunc")
bfs <- pbreplicate(nsim, simfunc(n=10, mu = 0, sd = 1), cl = cl)
parallel::stopCluster(cl)

mean(bfs > Q)

mu_critical <- qnorm(Q/2, 0)
pt(Inf, 9, ncp = 0 / (1 / sqrt(10))) - pt(mu_critical, 9, ncp = 0 / (1 / sqrt(10)))
```

## Examples

Using the equations defined above, we can again create a power function for Bayesian one-sided hypothesis evaluation. Note that for these hypotheses, the Bayes factor is defined irrespective of the fraction of information $J$ used to construct the prior. 

```{r}
BFpower <- function(n, mu, sd, mu0, Q, alternative = "greater") {
  mu_critical <- qnorm(Q/2, 0)
  alternative <- match.arg(alternative, c("greater", "less"))
  if (alternative == "greater") {
    pt(mu_critical, n-1, ncp = (mu - mu0) / sqrt(sd^2 / n), lower.tail = FALSE)
  } else if (alternative == "less") {
    pt(mu_critical, n-1, ncp = (mu - mu0) / sqrt(sd^2 / n), lower.tail = TRUE)
  }
}
```

### Changing the Bayes factor threshold

```{r}
Q <- 1.8
n <- 10
mu <- 0
sd <- 1
mu0 <- 0

cl <- parallel::makeCluster(16)
parallel::clusterExport(cl, c("simfunc", "Q", "n", "mu", "sd", "mu0"))
bfs <- pbreplicate(nsim, simfunc(n = n, mu = mu, sd = sd, mu0 = mu0), cl = cl)
parallel::stopCluster(cl)

mean(bfs > Q)
BFpower(n, mu, sd, mu0, Q, "greater")

Q <- 0.1
mean(bfs < Q)
BFpower(n, mu, sd, mu0, Q, "less")
```

### Changing $n$, $\mu$, $\sigma$ and $\mu_0$

```{r}
Q   <- 1.5
n   <- 30
mu  <- 0.5
sd  <- 2
mu0 <- 0.1

cl <- parallel::makeCluster(16)
parallel::clusterExport(cl, c("simfunc", "Q", "n", "mu", "sd", "mu0"))
bfs <- pbreplicate(nsim, simfunc(n = n, mu = mu, sd = sd, mu0 = mu0), cl = cl)
parallel::stopCluster(cl)

mean(bfs > Q)
BFpower(n, mu, sd, mu0, Q, "greater")
```

## Bayesian evaluation of range hypotheses

We now extend the power calculation to evaluating range hypotheses, in the sense of $\mu \in [R_l, R_u]$, where $R_l$ and $R_u$ denote the lower and upper bound of the range hypothesis. In this case, we assume that the prior distribution is centered around the center of the hypothesis space, such that the prior distribution is symmetric between the bounds of the hypothesis. Note that because the number of independent constraints in this hypothesis equals $2$, the minimal fraction of information used equals $\frac{1}{n/J}$, with $J$ at least equal to $2$. Accordingly, we can define the prior mean $\mu_0 = (R_l + R_u)/2$, and calculate the complexity of the hypothesis as
$$
c_i = 1 -  2 \cdot\Phi\Big(\frac{R_l - \mu_0}{\sqrt{\hat\sigma^2/J}}\Big).
$$
To determine the fit of the hypothesis, we cannot use this symmetry property, because the posterior distribution is not symmetric around the center of the hypothesis space (unless the posterior mean is exactly equal to the posterior mean). Instead, the fit of the hypothesis of interest is calculated as
$$
f_i = \Phi\Bigg(\frac{R_u - \hat\mu}{\sqrt{\hat\sigma^2/n}}\Bigg) - \Phi\Bigg(\frac{R_l - \hat\mu}{\sqrt{\hat\sigma^2/n}}\Bigg).
$$

### Sketch of the approach to take

Start from the assumption that $\hat\sigma^2$ is fixed, so that the complexity is determined a priori. Then, we can calculate the maximum Bayes factor that is obtainable ($1/c_i$), and we can obtain the probability that $f_i$ is greater by some value, by calculating how far from the bounds it must be to exceed some value. Say we fix $\hat\sigma^2 = 1$ and set $J=2$ and $R_l = -0.1$ and $R_u = 0.1$. Then, the complexity is defined as follows.

```{r}
1 - 2*pnorm(-0.1, 0, 1/2)
```
Moreover, to obtain a Bayes factor of $Q \geq 2$, the fit of the hypothesis must be at least $f_i \geq `r round(2 * (1 - 2*pnorm(-0.1, 0, 1/2)), 4)`$. Given that we again fix the variance $\hat\sigma^2 = 1$ and have $n = 20$, $\mu$ must lie between the following numbers to obtain a fit of $f_i \geq `r round(2 * (1 - 2*pnorm(-0.1, 0, 1/2)), 4)`$.

```{r}
mu <- 0
sigma <- 1
n <- 20


fi <- function(mu, varx, n, Rl, Ru) {
  pnorm(Ru, mu, sqrt(varx/n)) - pnorm(Rl, mu, sqrt(varx/n))
}

D <- uniroot(\(x) fi(x, sigma, n, -0.1, 0.1) - 2 * (1 - 2*pnorm(-0.1, 0, sigma/sqrt(2))), 
             interval = c(0, 10000))$root
```

We can again calculate the probability that $\hat\mu$ lies between $(R_l + R_u)/2 \pm D$ using a $t$-distribution. 

```{r}
bf <- function(meanx, varx, n, J = 1, Rl, Ru) {
  ci <- pnorm(Ru, (Rl+Ru)/2, sqrt(varx/(2*J))) - pnorm(Rl, (Rl+Ru)/2, sqrt(varx/(2*J)))
  fi <- pnorm(Ru, meanx, sqrt(varx/n)) - pnorm(Rl, meanx, sqrt(varx/n))
  
  c(ci = ci, fi = fi, BF = fi/ci)
}
bf(mean(x), var(x), n, 1, -0.1, 0.1)
bain::bain(fit, "x > -0.1 & x < 0.1", fraction = 1)$fit$BF.u[1]


bfs <- sapply(1:100000, \(i) {
  x <- rnorm(20)
  bf(mean(x), 1, 20, 1, -0.1, 0.1)
})

mean(bfs[3,] > 2)

pt(D*sqrt(20), n-1) - pt(-D*sqrt(20), n-1)
## But, because we fix the variance here, we can use a normal distribution, 
## instead of the t-distribution
pnorm(D*sqrt(20)) - pnorm(-D*sqrt(20))
```

So, using some simplifying assumptions, it is easy to calculate the power of the hypothesis of interest approximately. Once the variance is known, it is straightforward to calculate the power, also for different variances and sample sizes. However, we now note that not only the mean varies between samples, the estimated variance varies over the samples as well. 
The estimated variances affect both the complexity and the fit of the hypothesis, and thus introduces an additional complexity, because it affects both in different ways.


## A better approximation

To calculate the power of the Bayes factor of an informative hypothesis, we need to calculate the following triple integral
$$
\int_{-\infty}^{\infty}\int_0^{\infty}
\mathbb{I}\Bigg\{
\int_{R_l}^{R_u}
\frac{1}{\sqrt{2\pi\hat\sigma^2/n}} 
\exp\Big\{-\frac{(x - \hat\mu)^2}{2\hat\sigma^2/n}\Big\} \text{d}x ~~ \Big/ \\
\int_{R_l}^{R_u}\frac{1}{\sqrt{2\pi\hat\sigma^2/J}} 
\exp\Big\{-\frac{(x - \mu^*)^2}{2\hat\sigma^2/J}\Big\}
\text{d}x > T \Bigg\}
\text{d}\hat\sigma \text{d}\hat\mu,
$$
where $\mu^*$ is defined as $(R_l * R_u)/2$, $\mathbb{I}$ denotes the indicator function and $T$ defines the Bayes factor threshold. Additionally, we assume a normally distributed sample mean $\hat\mu$ with population mean $\mu$ and population variance $\sigma^2/n$ and a scaled chi-squared distributed sample variance $\hat\sigma^2$ with scaling factor $(n-1)\sigma^2$ and $n-1$ degrees of freedom (TODO: check whether formulation of variance is correct). 

We integrate over the sampling distributions of the mean and the variance. In compact notation, we can rewrite this as
$$
\int_{-\infty}^{\infty}\int_0^{\infty} \mathbb{I} \Big\{ f_i(\hat\mu, \hat\sigma, n)/c_i(\hat\sigma, J) > T \Big\} 
\text{d}\hat\mu \text{d}\hat\sigma,
$$
where we explicitly rewrite the fit and complexity as functions of the mean, variance, sample size and fraction of information used. 

### Integrating complexity over the variance

We establish that the complexity is a function of the variance only, and it's threshold is easily calculated using a chi-square distribution. Then, we can easily calculate the boundary value for which the complexity exceeds some threshold, and calculate the probability that the complexity exceeds this boundary value.

Below, we set the variance equal to $\sigma^2 = 1$ and $\sigma^2 = 2^2 = 4$ and evaluate the probability that the complexity is smaller than $0.15$ and $0.1$, respectively. 

```{r}
## And using variance 1
bfs <- sapply(1:1000000, \(i) {
  x <- rnorm(20)
  bf(mean(x), var(x), 20, 1, -0.1, 0.1)
})

mean(bfs[1,] < 0.15)

# Critical value
C <- uniroot(\(x) pnorm(0.1, 0, sqrt(x/2)) - pnorm(-0.1, 0, sqrt(x/2)) - 0.15,
             interval = c(0, 10))$root

1 - pchisq(C * 19, 19)


bfs <- sapply(1:1000000, \(i) {
  x <- rnorm(20, 0, 2)
  bf(mean(x), var(x), 20, 1, -0.1, 0.1)
})

mean(bfs[1,] < 0.1)

# Critical value
C <- uniroot(\(x) pnorm(0.1, 0, sqrt(x/2)) - pnorm(-0.1, 0, sqrt(x/2)) - 0.1,
             interval = c(0, 10))$root

1 - pchisq(C * 19 / 4, 19)
```

::: {.callout-tip title="Distribution of sampling variance"}

When the mean of the distribution is known, the sampling distribution distribution follows a scaled chi-squared distribution $\hat\sigma^2 \sim \sigma/n \cdot \chi^2_n$. When the mean of the distribution is unknown, the sampling distribution also follows a chi-squared distribution, but one degree of freedom is lost by calculating the sample mean, which yields $\hat\sigma^2 \sim \sigma/(n-1) \cdot \chi^2_{n-1}$.

```{r}
# distribution of the variance:
hist(replicate(100000, var(rnorm(20, 0, 2))), freq= FALSE, breaks = 50)
curve(dchisq(x * 19 / 4, 19) * 19/4, add = TRUE)
hist(replicate(100000, mean(rnorm(20, 0, 2)^2)), freq= FALSE, breaks = 50)
curve(dchisq(x * 20 / 4, 20) * 20/4, add = TRUE)
```
:::


### Integrating fit over the mean and variance

For the fit of the hypothesis, we also want to calculate the probability that some threshold is exceeded. This is a bit more difficult, because the fit is a function of both the variance and the mean. However, we can automate this using the `cubature` package, which calculates the integral numerically. To do this, we can define the distribution of the mean and the variance, and integrate the indicator function for the Bayes factors over these distributions. 


```{r}
library(cubature)

integrand <- \(musigma, threshold) {
  mu <- musigma[1]
  sigma <- sqrt(musigma[2])
  Fx <- pnorm(0.1, mu, sigma/sqrt(10)) - pnorm(-0.1, mu, sigma/sqrt(10))
  indicator <- 1 / (1 + exp(-100000 * (Fx - threshold)))
  
  dmu <- dnorm(mu, mean = 0, sd = 2/sqrt(10))
  dsigma <- dchisq(sigma^2 * 9/4, df = 9)*9/4
  
  indicator * dmu * dsigma
}

res <- adaptIntegrate(
  integrand, 
  lower = c(-Inf, 0),
  upper = c(Inf, Inf),#,
  threshold = 0.2
  #tol = 1e-3
)

mean(bfs[2,] > 0.2)
```

```{r}
library(cubature)

integrand <- \(musigmahat, threshold, mu, sigma, n) {
  df <- n-1
  muhat <- musigmahat[1]
  sigmahat <- sqrt(musigmahat[2])
  Fx <- pnorm(0.1, muhat, sigmahat/sqrt(n)) - 
    pnorm(-0.1, muhat, sigmahat/sqrt(n))
  indicator <- ifelse(Fx > threshold, 1, 0)

  dmu <- dnorm(muhat, mean = mu, sd = sigma/sqrt(n))
  dsigma <- dchisq(sigmahat^2 * df / sigma^2, df = df)*df/sigma^2
  
  indicator * dmu * dsigma
}


system.time(
  res <- adaptIntegrate(
    integrand, 
    lower = c(-Inf, 0),
    upper = c(Inf, Inf),
    threshold = 0.2, mu = 0, sigma = 2, n = 20
  )
)

res

mean(bfs[2,] > 0.2)
```

Unfortunately, this whole set-up is rather slow, perhaps because the indicator function yields a non-smooth function to be evaluated by the integral. To speed this up, we can also approximate the integral using a smooth function approximation to this indicator function, by specifying a steep logit curve at the threshold point, where the function moves quickly from zero to one. An alternative could be to lower the tolerance, but we use this approach first. 


```{r}
integrand <- \(musigmahat, threshold, mu, sigma, n) {
  df <- n-1
  muhat <- musigmahat[1]
  sigmahat <- sqrt(musigmahat[2])
  Fx <- pnorm(0.1, muhat, sigmahat/sqrt(n)) - 
    pnorm(-0.1, muhat, sigmahat/sqrt(n))
  indicator <- 1 / (1 + exp(-1000 * (Fx - threshold)))

  dmu <- dnorm(muhat, mean = mu, sd = sigma/sqrt(n))
  dsigma <- dchisq(sigmahat^2 * df / sigma^2, df = df)*df/sigma^2
  
  indicator * dmu * dsigma
}


system.time(
  res <- adaptIntegrate(
    integrand, 
    lower = c(-Inf, 0),
    upper = c(Inf, Inf),
    threshold = 0.2, mu = 0, sigma = 2, n = 20
  )
)

res

mean(bfs[2,] > 0.2)
```

This approach shows that we can get a reasonable approximation of the power of the Bayes factor using a smooth approximation to the indicator function. To further speed up the function evaluation, we can also lower the tolerance of the `adaptIntegrate()` function. 

### Integrating the Bayes factor over fit and complexity

Now we have everything in place to calculate the power of the Bayes factor over both the fit and complexity of the hypothesis of interest. In fact, we can simply expand the calculation of fit by incorporating the calculation of the complexity as well, which yields the power of the Bayes factor. We made some additional adaptations to the previous code to enhance computational efficiency, for example by vectorizing the computations. 

```{r}
integrand <- \(musigmahat, threshold, mu, var, n) {
  df <- n-1
  dfvar <- df/var
  sn <- sqrt(musigmahat[2,]/n)
  s2 <- sqrt(musigmahat[2,]/2)
  fit <- pnorm(0.1, musigmahat[1,], sn) - pnorm(-0.1, musigmahat[1,], sn)
  comp <- 1 - 2 * pnorm(-0.1, 0, s2)
  ind <- 1 / (1 + exp(-1000 * (fit/comp - threshold)))
  matrix(ind * dnorm(musigmahat[1,], mu, sqrt(var/n)) * 
    dchisq(musigmahat[2,]*dfvar, df) * dfvar,
    ncol = ncol(musigmahat))
}

system.time(
  res <- adaptIntegrate(
    integrand, 
    lower = c(-Inf, 0),
    upper = c(Inf, Inf),
    threshold = 2, mu = 0, var = 2, n = 20,
    vectorInterface = TRUE
  )
)

res

mean(bfs[3,] > 2)
```

Now we are able to calculate the power of the Bayes factor for a given sample size on the basis of the mean and variance of the data, without having to sample from any distribution.

We can now easily use R's built-in root-finding algorithm to compute for which sample size the power of the Bayes factor exceeds some threshold. 

```{r}
power <- 0.8

fx <- \(x) {
  adaptIntegrate(
    integrand,
    lower = c(-Inf, 0),
    upper = c(Inf, Inf),
    threshold = 2, mu = 0, var = 2, n = x,
    vectorInterface = TRUE
  )$integral - power
}

fx(20)

uniroot(fx, c(5, 1000))
```



# Loose ends

Now we have defined Bayes power analysis for the one-sample $t$-test under the approximate adjusted Bayes factor set-up. However, there are some loose ends that still need to be tied up.

1. A follow-up question is how to calculate the power of the Bayes factor when two informative hypotheses are evaluated against each other. Currently, we only evaluated against the unconstrained hypothesis. This is a more difficult problem, as we have to take into account that the Bayes factor is dependent on quantities in both the numerator and the denominator. Presumably, this is just an extension of the work already done, either by calculating the critical value in a different way, or by letting `cubature` integrate over an additional distribution as well. 
2. We have not yet investigated the power of the Bayes factor under a multi-parameter set-up. Yet, this is possible using a (non-central) $\chi^2$-distribution or a (non-central) $F$-distribution, at least for null hypotheses. For informative hypotheses, the problem might be more complex.




<!-- # Power of a Bayesian two-sample t-test -->

<!-- Now, consider the situation that we have two samples $\{X_i^{(1)}\}_{i=1}^{n_1}$ and $\{X_j^{(2)}\}_{j=1}^{n_2}$, one of size $n_1$ and one of size $n_2$. Let's assume that $n_1 = n_2$ for simplicity. Moreover, assume that we want to calculate the Bayes factor for the null hypothesis that the means of the two samples are equal, against the alternative hypotheses that the means are not equal. That is, we have $\mathcal{H}_0: \mu_1 = \mu_2$ and $\mathcal{H}_1: \mu_1 \neq \mu_2$. For the time being, also assume that they have the same variance.  -->

<!-- Accordingly, we can again use the Savage-Dickey density ratio to calculate the Bayes factor, but now using a multivariate normal distribution. Hence, under the set-up of the approximate adjusted fractional Bayes factor, we have that the Bayes factor is given by -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- BF_{0,1} &= \frac{P(\mu_1 = \mu_2 | X)}{P(\mu_1 = \mu_2 | \pi_0)} -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- set.seed(1) -->

<!-- n1 <- 20 -->
<!-- n2 <- 20 -->

<!-- mu1 <- 0 -->
<!-- mu2 <- 1 -->

<!-- sigma1 <- 1 -->
<!-- sigma2 <- 1 -->

<!-- x1 <- rnorm(n1, mu1, sigma1) -->
<!-- x2 <- rnorm(n2, mu2, sigma2) -->

<!-- fit <- bain::t_test(x2, x1, var.equal = TRUE) -->

<!-- bf <- bain::bain(fit, 'x = y', fraction = 2) -->
<!-- bf -->

<!-- sp <- sqrt((n1 - 1)*var(x1) + (n2 - 1)*var(x2)) / sqrt(n1 + n2 - 2) -->

<!-- dnorm(0, mean(x2) - mean(x1), sp * sqrt(1/n1 + 1/n2)) / -->
<!--   dnorm(0, 0, sp / 3) -->
<!-- ``` -->


<!-- ## Sketch of the approach to take -->

<!-- Find the radius of the ellipsoid that corresponds to the critical value of the Bayes factor. Then, calculate the probability that the sample mean falls within this ellipsoid (and note that the sample mean follows a multivariate $t$ distribution). -->

<!-- ```{r} -->
<!-- n <- 1000 -->
<!-- mu <- c(1,2) -->
<!-- sigma <- diag(c(1,2) - 0.5) + 0.5  -->

<!-- x <- matrix(rnorm(n*2), n) %*% chol(sigma) + matrix(mu, n, 2, byrow = TRUE) -->

<!-- library(ggplot2) -->

<!-- angles <- (0:256) * 2 * pi / 256 -->
<!-- unit.circle <- cbind(cos(angles), sin(angles)) -->
<!-- ellipse <- t(c(0, 0) + 1 * t(unit.circle %*% chol(J * sigma))) -->

<!-- data.frame(x) |> -->
<!--   ggplot(aes(x = X1, y = X2)) + -->
<!--   geom_point() + -->
<!--   stat_density2d() + -->
<!--   geom_path(data = data.frame(ellipse), col = "orange") -->

<!-- ``` -->

<!-- For calculation of the ellipse, see  -->
<!-- [https://ggplot2.tidyverse.org/reference/stat_ellipse.html](https://ggplot2.tidyverse.org/reference/stat_ellipse.html) -->

<!-- For some notes on the geometry of the multivariate Gaussian distribution, see -->
<!-- [chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://cs229.stanford.edu/section/gaussians.pdf](chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://cs229.stanford.edu/section/gaussians.pdf) -->

<!-- And for some notes on the numerical computation of multivariate normal and multivariate $t$ probabilities over ellipsoidal regions, see [chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/file:///C:/Users/5868777/AppData/Local/Google/Chrome/Downloads/jss2r.pdf](chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/file:///C:/Users/5868777/AppData/Local/Google/Chrome/Downloads/jss2r.pdf) -->

<!-- Other links: -->

<!-- - [https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/#comment-190](https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/#comment-190) -->

<!-- ```{r} -->
<!-- n <- 10000 -->
<!-- mu <- c(1,2) -->
<!-- sigma <- diag(c(1,2) - 0.5) + 0.5 -->

<!-- X <- mvtnorm::rmvnorm(n = n, mean = mu, sigma = sigma) -->
<!-- var(X) -->
<!-- colMeans(X) -->

<!-- C <- X - t(colMeans(X)) %x% rep(1, n) -->
<!-- S <- C %*% solve(chol(var(C))) -->

<!-- all.equal(X, S %*% chol(var(C)) + t(colMeans(X)) %x% rep(1, n)) -->
<!-- ``` -->



<!-- ```{r} -->

<!-- fx <- function(x, mu, sigma) { -->
<!--   k <- length(mu) -->
<!--   exp(-t(x-mu) %*% solve(sigma) %*% (x-mu) / 2) / sqrt((2*pi)^k * det(sigma)) -->
<!-- } -->

<!-- fx(c(1,2), mu, sigma) -->
<!-- mvtnorm::dmvnorm(c(1,2), mu, sigma) -->

<!-- t(c(2,0)) %*% solve(sigma) %*% c(2,0) -->
<!-- eigen(sigma) -->
<!-- ``` -->

<!-- ### Forming the hyperellipse around $\mathbf{0}_p$ -->

<!-- ```{r} -->
<!-- set.seed(1) -->
<!-- P <- 3 -->
<!-- N <- 10000000 -->
<!-- S <- 0.7 + 0.3 * diag(P) -->
<!-- M <- rep(0, P) -->
<!-- # X <- mvtnorm::rmvt(N, sigma = S, delta = M, df = 10-P) -->
<!-- # For normal distribution, uncomment the following two lines -->
<!-- X <- rnorm(N*P) |> matrix(N) %*% chol(0.7 + 0.3 * diag(P)) -->
<!-- X <- X + t(M) %x% rep(1, N) -->
<!-- rgl::plot3d(X, xlim = c(-10,10), ylim = c(-10,10), zlim = c(-10,10)) -->
<!-- #rgl::text3d(X, texts = 1:N, add = TRUE) -->
<!-- rgl::plot3d(rgl::ellipse3d(var(X), subdivide = 2^2), col = "seagreen", alpha = 0.3, add = TRUE) -->

<!-- E <- eigen(var(X)) -->
<!-- E <- eigen(S) -->
<!-- #XC <- X - t(colMeans(X)) %x% rep(1, N) -->
<!-- XC <- X -->
<!-- C <- qchisq(0.95, P) -->
<!-- #C <- 3 * qf(0.95, P, 10-P, ncp = 0) -->
<!-- C -->
<!-- #C <- 1 -->
<!-- W <- diag(1 / sqrt(C * E$values)) %*% t(E$vectors) -->

<!-- mean(colSums((W %*% t(XC))^2) < 1) -->
<!-- pchisq(1, 0.95, P) -->

<!-- shotGroups::pmvnEll(sqrt(C), sigma = 0.7 + 0.3 * diag(P), mu = M,  -->
<!--                     e = solve(0.7 + 0.3 * diag(P)), x0 = rep(0,P)) -->

<!-- mvtnorm::qmvnorm(0.95, mean = rep(0,P), sigma = 0.7 + 0.3 * diag(P),  -->
<!--                  lower = rep(-Inf, P), upper = rep(Inf, P)) -->

<!-- mvtnorm::pmvnorm(-2, 2, mean = rep(0,P), sigma = 0.7 + 0.3 * diag(P), -->
<!--                  ) -->

<!-- L <- t(chol(S)) -->
<!-- E2 <- eigen(L %*% S %*% t(L)) -->
<!-- t(E2$vectors) %*% solve(L) %*% rep(0, P) -->
<!-- ``` -->

